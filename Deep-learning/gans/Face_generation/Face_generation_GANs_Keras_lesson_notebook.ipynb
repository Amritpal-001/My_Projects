{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mB_sFildiDh"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnWq-E3_V64s"
   },
   "source": [
    "youtube video - https://www.youtube.com/watch?v=T-MCludVNn4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "303iHntmdiDj"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 7: Generative Adversarial Networks**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5n2iv9udiDk"
   },
   "source": [
    "# Module 7 Material\n",
    "\n",
    "* Part 7.1: Introduction to GANS for Image and Data Generation [[Video]](https://www.youtube.com/watch?v=0QnCH6tlZgc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_1_gan_intro.ipynb)\n",
    "* **Part 7.2: Implementing a GAN in Keras** [[Video]](https://www.youtube.com/watch?v=T-MCludVNn4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_2_Keras_gan.ipynb)\n",
    "* Part 7.3: Face Generation with StyleGAN and Python [[Video]](https://www.youtube.com/watch?v=Wwwyr7cOBlU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_3_style_gan.ipynb)\n",
    "* Part 7.4: GANS for Semi-Supervised Learning in Keras [[Video]](https://www.youtube.com/watch?v=ZPewmEu7644&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_4_gan_semi_supervised.ipynb)\n",
    "* Part 7.5: An Overview of GAN Research [[Video]](https://www.youtube.com/watch?v=cvCvZKvlvq4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_5_gan_research.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1602695371768,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "zgMWBuf61OmL"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yqlUD4sdiDk"
   },
   "source": [
    "# Part 7.2: Implementing DCGANs in Keras\n",
    "\n",
    "Paper that described the type of DCGAN that we will create in this module. [[Cite:radford2015unsupervised]](https://arxiv.org/abs/1511.06434) This paper implements a DCGAN as follows:\n",
    "\n",
    "* No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1]. \n",
    "* All models were trained with mini-batch stochastic gradient descent (SGD) with a mini-batch size of 128. \n",
    "* All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. \n",
    "* In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\n",
    "* we used the Adam optimizer(Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. \n",
    "* Additionally, we found leaving the momentum term $\\beta{1}$ at the suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.\n",
    "\n",
    "The paper also provides the following architecture guidelines for stable Deep Convolutional GANs:\n",
    "\n",
    "* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "* Use batchnorm in both the generator and the discriminator.\n",
    "* Remove fully connected hidden layers for deeper architectures.\n",
    "* Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "* Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n",
    "While creating the material for this module I used a number of Internet resources, some of the most helpful were:\n",
    "\n",
    "* [Deep Convolutional Generative Adversarial Network (TensorFlow 2.0 example code)](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
    "* [Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)\n",
    "* [Collection of Keras implementations of Generative Adversarial Networks GANs](https://github.com/eriklindernoren/Keras-GAN)\n",
    "* [dcgan-facegenerator](https://github.com/platonovsimeon/dcgan-facegenerator), [Semi-Paywalled Article by GitHub Author](https://medium.com/datadriveninvestor/generating-human-faces-with-keras-3ccd54c17f16)\n",
    "\n",
    "The program created next will generate faces similar to these.  While these faces are not perfect, they demonstrate how we can construct and train a GAN on or own.  Later we will see how to import very advanced weights from nVidia to produce high resolution, realistic looking faces. Figure 7.GAN-GRID shows images from GAN training.\n",
    "\n",
    "**Figure 7.GAN-GRID: GAN Neural Network Training**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan-3.png \"GAN Images\")\n",
    "\n",
    "As discussed in the previous module, the GAN is made up of two different neural networks: the discriminator and the generator.  The generator generates the images, while the discriminator detects if a face is real or was generated.  These two neural networks work as shown in Figure 7.GAN-EVAL:\n",
    "\n",
    "**Figure 7.GAN-EVAL: Evaluating GANs**\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_1.png \"GAN\")\n",
    "\n",
    "The discriminator accepts an image as its input and produces number that is the probability of the input image being real.  The generator accepts a random seed vector and generates an image from that random vector seed. An unlimited number of new images can be created by providing additional seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpCjlQyEdiDo"
   },
   "source": [
    "I suggest running this code with a GPU, it will be very slow on a CPU alone.  The following code mounts your Google drive for use with Google CoLab.  If you are not using CoLab, the following code will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 41607,
     "status": "ok",
     "timestamp": 1602695415696,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "Y8_-1h5ddiDp",
    "outputId": "ece36a97-5519-4085-88e1-d6dfe4bc32aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Note: using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3098,
     "status": "ok",
     "timestamp": 1602695418799,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "KubxTY1mdiDm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9uwIRYJdiDr"
   },
   "source": [
    "These are the constants that define how the GANs will be created for this example.  The higher the resolution, the more memory that will be needed.  Higher resolution will also result in longer run times.  For Google CoLab (with GPU) 128x128 resolution is as high as can be used (due to memory).  Note that the resolution is specified as a multiple of 32.  So **GENERATE_RES** of 1 is 32, 2 is 64, etc.\n",
    "\n",
    "To run this you will need training data.  The training data can be any collection of images.  I suggest using training data from the following two locations.  Simply unzip and combine to a common directory.  This directory should be uploaded to Google Drive (if you are using CoLab). The constant **DATA_PATH** defines where these images are stored.\n",
    "\n",
    "The source data (faces) used in this module can be found here:\n",
    "\n",
    "* [Kaggle Faces Data New](https://www.kaggle.com/gasgallo/faces-data-new)\n",
    "* [Kaggle Lag Dataset: Dataset of faces, from more than 1k different subjects](https://www.kaggle.com/gasgallo/lag-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3041,
     "status": "ok",
     "timestamp": 1602695462904,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "_-VMKeuqYJKf"
   },
   "outputs": [],
   "source": [
    "!unzip -q  '/content/drive/My Drive/projects/FaceData8000.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1602700165040,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "tb_XblE7diDr",
    "outputId": "1bc914f4-9bb7-436f-d090-b84fd6b83050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will generate 32px square images.\n"
     ]
    }
   ],
   "source": [
    "# Generation resolution - Must be square \n",
    "# Training data is also scaled to this.\n",
    "# Note GENERATE_RES 4 or higher  \n",
    "# will blow Google CoLab's memory and have not\n",
    "# been tested extensivly.\n",
    "GENERATE_RES = 1 # Generation resolution factor \n",
    "# (1=32, 2=64, 3=96, 4=128, etc.)\n",
    "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Preview image \n",
    "PREVIEW_ROWS = 9\n",
    "PREVIEW_COLS = 9\n",
    "PREVIEW_MARGIN = 16\n",
    "\n",
    "# Size vector to generate images from\n",
    "SEED_SIZE = 50\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '/content/FaceData8000'\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 60000\n",
    "\n",
    "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDTfFQjTdiDu"
   },
   "source": [
    "Next we will load and preprocess the images.  This can take awhile.  Google CoLab took around an hour to process.  Because of this we store the processed file as a binary.  This way we can simply reload the processed training data and quickly use it.  It is most efficient to only perform this operation once.  The dimensions of the image are encoded into the filename of the binary file because we need to regenerate it if these change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1602700165041,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "dJ69ALfSdiDv",
    "outputId": "cd737930-6bfe-4188-c548-13df30ce0804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: /content/FaceData8000/training_data_32_32.npy\n",
      "Loading previous training pickle...\n"
     ]
    }
   ],
   "source": [
    "# Image set has 11,682 images.  Can take over an hour \n",
    "# for initial preprocessing.\n",
    "# Because of this time needed, save a Numpy preprocessed file.\n",
    "# Note, that file is large enough to cause problems for \n",
    "# sume verisons of Pickle,\n",
    "# so Numpy binary files are used.\n",
    "training_binary_path = os.path.join(DATA_PATH,\n",
    "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
    "\n",
    "print(f\"Looking for file: {training_binary_path}\")\n",
    "\n",
    "if not os.path.isfile(training_binary_path):\n",
    "  start = time.time()\n",
    "  print(\"Loading training images...\")\n",
    "\n",
    "  training_data = []\n",
    "  faces_path = os.path.join(DATA_PATH)\n",
    "  for filename in tqdm(os.listdir(faces_path)):\n",
    "      path = os.path.join(faces_path,filename)\n",
    "      \n",
    "      image = Image.open(path).resize((GENERATE_SQUARE,\n",
    "            GENERATE_SQUARE),Image.ANTIALIAS)\n",
    "      training_data.append(np.asarray(image))\n",
    "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
    "            GENERATE_SQUARE,IMAGE_CHANNELS))\n",
    "  training_data = training_data.astype(np.float32)\n",
    "  training_data = training_data / 127.5 - 1.\n",
    "\n",
    "\n",
    "  print(\"Saving training image binary...\")\n",
    "  np.save(training_binary_path,training_data)\n",
    "  elapsed = time.time()-start\n",
    "  print (f'Image preprocess time: {hms_string(elapsed)}')\n",
    "else:\n",
    "  print(\"Loading previous training pickle...\")\n",
    "  training_data = np.load(training_binary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9kO_iSRyixQ"
   },
   "source": [
    "We will use a TensorFlow **Dataset** object to actually hold the images.  This allows the data to be quickly shuffled int divided into the appropriate batch sizes for training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1897,
     "status": "ok",
     "timestamp": 1602700165722,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "Hha03HCsOCC3",
    "outputId": "4ef55114-fb3d-453c-d510-133de777d46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/content/FaceData8000/training_data_96_96.npy': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv '/content/FaceData8000/training_data_96_96.npy' '/content/training_data_96_96.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1602700165723,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "BXl0JohJBx69"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(training_data) \\\n",
    "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dATyXqQdiDw"
   },
   "source": [
    "The code below creates the generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB_aX4ChdiD0"
   },
   "source": [
    "Next we actually build the discriminator and the generator.  Both will be trained with the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1890,
     "status": "ok",
     "timestamp": 1602700165723,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "Ulou-BZPybzT"
   },
   "outputs": [],
   "source": [
    "def build_generator(seed_size, channels):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
    "    model.add(Reshape((4,4,256)))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    if GENERATE_RES>1:\n",
    "      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
    "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "      model.add(BatchNormalization(momentum=0.8))\n",
    "      model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Final CNN layer\n",
    "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n",
    "                     padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kxKk7uDOnPS"
   },
   "source": [
    "As we progress through training images will be produced to show the progress.  These images will contain a number of rendered faces that show how good the generator has become.  These faces will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 1886,
     "status": "ok",
     "timestamp": 1602700165724,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "UKnCeDut2cp0"
   },
   "outputs": [],
   "source": [
    "def save_images(cnt,noise):\n",
    "  image_array = np.full(( \n",
    "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
    "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
    "      255, dtype=np.uint8)\n",
    "  \n",
    "  generated_images = generator.predict(noise)\n",
    "\n",
    "  generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "  image_count = 0\n",
    "  for row in range(PREVIEW_ROWS):\n",
    "      for col in range(PREVIEW_COLS):\n",
    "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
    "            = generated_images[image_count] * 255\n",
    "        image_count += 1\n",
    "\n",
    "          \n",
    "  output_path = '/content/drive/My Drive/projects/output'\n",
    "  if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "  \n",
    "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
    "  im = Image.fromarray(image_array)\n",
    "  im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiUbj3W4Oo3U"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 1882,
     "status": "ok",
     "timestamp": 1602700165725,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "gL5byGhNzOzd",
    "outputId": "b0b08f05-5f66-4d74-fe15-8792a89f1fa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8c1edd1e10>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfd0lEQVR4nO2de4yc53XenzOXnb3M7I27JJcXcUWZutuiJFaRY9dV7EZV7TSy28Cw2xoCalipYQM2kKIRXCB2gQJ12tqGkRQO5FiI3Lq+1LZiIXFaqYpg1U1CibZ1JyVRFG+7y93lcu879zn9Y4YAJbzPu0sud5b29/wAgrvv2fN9Z975znwz7zPnvObuEEL86pPa7ACEEO1ByS5EQlCyC5EQlOxCJAQluxAJQckuRELIrMfZzO4B8FUAaQB/6u5fjP19Z3+n50fyQVupzkOplonNIrFVudEa3K8RmxHy0mhV7uJpbrMOHojXePzpiF+9Tvw8MlmNiC3Nz2URNxp/7PYSecwWU4gjz6ez88Wunci5PM2NluG2TLpObd2Z8AW0VO2gPp2ZWnB8eWIR5blS8NFdcrKbWRrAfwXwmwBOA3jGzB5195eZT34kjw88/NtB22tzw/RcY28MhQ2Rye04wx9adpk/06UhfuU0OsPn65zkGV0tRC6Aq5d4HDNd1NY/skBtC4thP2cvAgCwmKWm9GCZ2zL8Ai6fC8dhndwnNcPjSFV4/JnI81nNk/mPvOhYOI+axxvk8XcMlKhtqI8/17cPnQqO/3R8L/W5fstUcPyxf/UI9VnP2/g7ABx192PuXgHwHQD3ruN4QogNZD3JvhPAhS9Jp1tjQogrkA1foDOz+83skJkdKs3xtzlCiI1lPck+BmD3Bb/vao29CXd/0N0PuPuBzv7OdZxOCLEe1pPszwDYZ2ZXm1kHgI8AePTyhCWEuNxc8mq8u9fM7NMA/jea0ttD7v5SzKc7XcFthZNB29NjV1G/7CxZ7Y4sMNciq+AeedSxVd/6lrBEUuJCQlTyqi3kqK13+yK17emfpbZxskJeyPFV9UYkxnqD3w+KVb56zmS5Siky+REJsNbNn8/6bv7xsK93JTj+6yPHqc94sZfa8lk+jxMrfdSWAo+/QfTBvQMz1OdTI08Ex5/NcqVmXTq7u/8YwI/XcwwhRHvQN+iESAhKdiESgpJdiISgZBciISjZhUgI61qNv+iTWR3DmbA0UClzGYdFmeIqCBrdvGAhs8QftkUKRnqfDUtl2WUuq1R6IwUcK/wxL13VT22vXssLbxpEvopJb3dtfY3aJitchlqo8i9J/d3x0eB4o8pjty5ehJSd4/elwgs8juJwuCDnL/cVqE/6HL8+0rvDUh4ApCMVguUSf66XSXVbV5aXUz62+Pbg+EKdy3W6swuREJTsQiQEJbsQCUHJLkRCULILkRDauhrfcEPJw6uS12yfpn6vLO8gB+TnivV3q2yL9B3KcL/Os6RwJdbCLaIYxPxitpF+XuzAClfOLXdTn9e6t1JbTyb2ADg93WG/q3dOUJ9ija9YTy2FexcCwFwXVy7qfWRFu8Lvc5llbqvVuJpQP83nOB0psBqvhc+XyvJrcbgz3Oaq1Ii09qIWIcSvFEp2IRKCkl2IhKBkFyIhKNmFSAhKdiESQnulN6SwWA8XJvzr3T+hfi8O7QqOPzF5HfX5pzuepbbjpS3U9uTYPmor58PSW6ZIXdB5jssnuXlerFPcyiWUk1OD/IQEH48UrSzx3WcyWS5Tluci3YJJbdCRyiVs8wVgcGCZ2lI7eXFKX0+4P938IpfJqiv8HmiRvaEapEchADQWI8VXc+HnOrbj1Us924Pjsb6AurMLkRCU7EIkBCW7EAlByS5EQlCyC5EQlOxCJIR1SW9mdhzAIoA6gJq7H4j9fSFVwnt7jgRtgykuQ9XJ9jiPNW6gPifLXJ6qR17jdvXNU9sLo+F+bF1TfBpLg/xc83u5rbSdS17b+sMVTwAwdTQsK+Yn+LlWRqgJ/+RtL1LbqeIAte3snOMHJfz16WupbU/fOWp7dn43tTEaVT4fvW9EnrN8pFdiRCtLl3nVm5NCus5pHkf++nBVYTrFpd7LobP/hrufvQzHEUJsIHobL0RCWG+yO4DHzOxnZnb/5QhICLExrPdt/LvdfczMtgJ43MyOuPtTF/5B60XgfgAY2cm7fAghNpZ13dndfaz1/xSARwDcEfibB939gLsfGIgsVgkhNpZLzj4z6zGzwvmfAdwNgC/dCiE2lfW8jd8G4BEzO3+c/+Hu/+tSD/Yfp36D2n46vjc4vvgyl9ceuaqP2q4dmaK24zP8mJnZ8HQNHuZVV9UCl2rK/Xz6+yLyz5kcb7CYmw1/VCpujdVQcf526mpqGzvN5yrdHZYO06/zCrvOc1yeemELl/lyy9xv9vqwLTVFmocCKIxxGTgdqcyLbQPWyPIYV7aHbdU8P95AZ7jUMp3iPpec7O5+DMAtl+ovhGgv+hAtREJQsguREJTsQiQEJbsQCUHJLkRCaGvDyRpSmGmEpZcaK/0B0NsZrvCZG+H7kO0Znr244FoUp3kjwpFfhGUN+5vnqE/33lFua/AKJe/gkt3i7m3UtuWlsORV3MLnd7qXn6vSz/0Gti5S23Lx4vfFS1W4LV3kjpEekMBS+LHlIjKf1fkBcwvc1nmWN5z0ND9fqhZOw8Wr+L24Wg8/Lx6ZC93ZhUgISnYhEoKSXYiEoGQXIiEo2YVICG1djTc40givQBcy4W16AODmgYng+MlX+Kr08dJWHkg2UiywyFefG8QvvY2fq7KDF62kKrzgojzMt1ZqdFAT6p3hVd/iNr4afN11Y9SWifQ0i/GPrnk5OP5XW2+iPhML4R5/ALA30hvwyDOj1Ga94SX+RpYX5JT7+DWQXY70eFvmfQNLW3nhTbUnIlEQcpnwuVIRaUJ3diESgpJdiISgZBciISjZhUgISnYhEoKSXYiE0FbprdzI4vVKWKZ6bnYn9btlICwN5bbz3m+leS512AqXVlK8tgbFofBr4+x7wz3yACBT5lJIpYe/1qYixRhEvWyaMmEZp2OeH+/V01zCTGX4yYYHeCHM66Xh4PhKleuG//yaZ6jtyBLfo+rl7siE1MNz3DPB5yO7wo+XjjyfqQqX3rJLPNVy8+EYa0RGBYDXpsLzW6ryoibd2YVICEp2IRKCkl2IhKBkFyIhKNmFSAhKdiESwqrSm5k9BOC3AEy5+82tsUEA3wUwCuA4gA+7+6pN36pIY7IW3pYpJskU62E5obTA5bXOU/x4XVORqrfwrjoAgO7pcI+xpR18Gus5Lp/kx3nPsqUdXELJLlET6h3h8zFJDgDsLJ+rxiCP8cw435Lpr8vhY7rzOL7x8q9TW3mWVwFajR8zPRWOo2ORXwO1yHOWqnE/q3HJLjvPNd1UOVz9mKrx67vcFT5eOlKluJY7+58BuOctYw8AeMLd9wF4ovW7EOIKZtVkb+23fu4tw/cCeLj188MAPniZ4xJCXGYu9TP7Nnc/31HiDJo7ugohrmDWvUDn7g6AfpAxs/vN7JCZHVo+F2kMLoTYUC412SfNbAQAWv/TDc/d/UF3P+DuB3oGI/2UhBAbyqUm+6MA7mv9fB+AH12ecIQQG8VapLdvA7gLwJCZnQbweQBfBPA9M/s4gBMAPryWk5UbGRxdCVe9TcyEJTkA+MCOF4PjhS3L1Gd5gT+0wgkurVTz3FYphavlGpdYO7iwJ1KhxBUvdE9FJJ5i2FbP8ko/j8hr2S5uqxYjW1RN54PjsYpDL/Cqsdwkn+TyVt64s+f1sF9ujp8rtlVWrFKxdgO/hsu9F99UshZpRPmubSeD4yez/KPyqpepu3+UmN63mq8Q4spB36ATIiEo2YVICEp2IRKCkl2IhKBkFyIhtLXhZN1TmK+G99hKvc733vpW94Hg+OJEgfp0LHHZYmU7t5WGuaw1dyuRa6r8NTNdiGhoE5GqvWl+zOwij581SyxviVS9RZpKVpb4F6Ey01x6AzldZiUy972RJpCR53PgCJ+r/uffWtbRxBr8MS9ctYXaUlyxQ40rdsgu88eWHwvLZaf+Ib8+hjvCzT6zxmVI3dmFSAhKdiESgpJdiISgZBciISjZhUgISnYhEkJbpbdaI4W5clhiy960QP2u6p8Ljs/keIXPZGOI2hq5yB5r5YhE1RXWXerLXCLxKW5L1fm5YpV0c/u5nDd/Y/ixeUQz6u7i81gCl96yEVmONcXsnOESlKf58fqPckmpEZG8ytt7wuMDfIJj0uzgYR4HjPt1zvDnzFNhP38br+q8qet0+DyRcknd2YVICEp2IRKCkl2IhKBkFyIhKNmFSAhtXY3vzZbwvq1HgrbqEA9lV8dMcPym3Dj1+WTjX1DbuZf5Sn29lxdI9PWUguMLDb4Ke/touFcYAIwt8Z5lpSqfj0/uPUhtr6xsD44/fuw66vP27RPU9vTRUWqrdfOV9W5yyI6lSP+8Jb6svryV35dKkSKfznNhv0pfpCBnF1/RLk7z5yVWJFPv4EVDbLup9+97jvrcnhsLjvcYV1Z0ZxciISjZhUgISnYhEoKSXYiEoGQXIiEo2YVICGvZ/ukhAL8FYMrdb26NfQHAJwBMt/7sc+7+49VP1sBgOvzl/uEcL4QpeVi2qDqXan579wvU9pepm6htdyFcdAMAHalwEcRSPy922dE1T23zpCgIAO4eCUuUAPB6aZjaTq4MBMerk/xcT0/to7a+V/gcdyxw6a0arj9BnchMAFAa5MerdXG/2PZP6Wo4/mqBn8vKkf5/pMAHAIrDPMbYdl4Noso9fpzLpbf1HA+OLzXCvemAtd3Z/wzAPYHxr7j7/ta/VRNdCLG5rJrs7v4UgHCLTiHELw3r+cz+aTN73sweMrPwe0chxBXDpSb71wBcA2A/gAkAX2J/aGb3m9khMzu0OBv54CKE2FAuKdndfdLd6+7eAPB1AHdE/vZBdz/g7gcKA5FNBYQQG8olJbuZjVzw64cAvHh5whFCbBRrkd6+DeAuAENmdhrA5wHcZWb7ATiA4wB+dy0nK3kWr5bCVVnP1XdTv+u6zwTHR7Nnqc+JIt/CZ0/vLLWdWuyntmo9LOOkjMs450rd/Fzjg9R2+yCvlvvIAK96yw6GZaj7F/4l9Vn+BZ8r8CI11Liah3pHWIZqpGOVcly66p7igUzn+T2rTOS8+s5wBSMAbBngvd+me/PU1pUvU1tk1yv82o4TwfGeDD8e6zVnkWtx1WR3948Ghr+xmp8Q4spC36ATIiEo2YVICEp2IRKCkl2IhKBkFyIhtLXhpMGRtbA0lM3wyqU/n9gfHH88cyP1mS6SsisAewpcetuZ51VqrILtyPw26vOxnX9Hbd/q+DUeR47HWAGvROuzsFzDZEMAKG/jnRJTVf5FqPwpakJ+LHxM88j2T2QbJACwiAToETmvYyZ8P2ssc91wMcNtVuCBlCNbfWFnkZqOL4Ul2N8f/Svqc002fH2o4aQQQskuRFJQsguREJTsQiQEJbsQCUHJLkRCaKv0Vm2kMVYKV5UNZnml0WI5LGkcPbmVn6zMpSYb5W6jvbwD118ceXtwvHGWSy5/MM5j3L2Vy2vpSLnZ92a4ZDde7A2Or7zGq/k6uFqDntNc1ipH9kurFMKX1vIufrxGLlIRN8bvS57mc1XpDx8zXYzE3s+Pd9UN4QpMAOjM8OYs/TkuvX1i+0+C43d18uMdIY00G4g0vaQWIcSvFEp2IRKCkl2IhKBkFyIhKNmFSAhtXY3vSldwSyFcPbFU76R+twyNhQ1sHMDjR26gtt4c7z9WafBV/Po8KQrJ80KSkSFeWHN17wy1bc9yv5MZ3jMu0xMuKHqR1xlFV6YR6WlW3MZt9e6wLXeW3186xngcXWf5Cnm1EFlZHwj71QrUBSjwVfDlSge1nZrk2yf09vLV+L8thLffKqT4FmaPzocbOp+rP0l9dGcXIiEo2YVICEp2IRKCkl2IhKBkFyIhKNmFSAhr2f5pN4BvAtiG5nZPD7r7V81sEMB3AYyiuQXUh92dV3YAyKCBwfRS0PbqSnhbKADY2xXe5mlXBy9aeWw5XLQCAEdO8XOlprm00jUbfm2s9XAJaq6f9zN7I8UltH/7xj+jtnwPlw6Xi+GinC3PRwpQuNqIwim+BVHhNL9XTL8jPI/ZJR5HzySX11JV7peq8MvYiSyaYjIqgPQ4L2yaWeR+qRU+H0uRPnk/PXtNcPxA9xvU5+au08HxrtT6etDVAPyeu98I4E4AnzKzGwE8AOAJd98H4InW70KIK5RVk93dJ9z9562fFwEcBrATwL0AHm792cMAPrhRQQoh1s9FfWY3s1EAtwI4CGCbu0+0TGfQfJsvhLhCWXOym1kewA8AfNbdFy60ubuj+Xk+5He/mR0ys0OLs/xriEKIjWVNyW5mWTQT/Vvu/sPW8KSZjbTsIwCmQr7u/qC7H3D3A4UBvrghhNhYVk12MzM092M/7O5fvsD0KID7Wj/fB+BHlz88IcTlYi1Vb+8C8DEAL5jZs62xzwH4IoDvmdnHAZwA8OHVDjRb68b3Jw8EbS/9bJT6jdwYfNOA8TO8yih/jD+0eie3kd2pAABponiVB7msknFekTU+00dtnV1cQimWuTxYG+sOjhfe4FVX1T5+PKtxOaxjice4/ZnwRBaHI9LmJJcUV3bwqsjiLl51ODi0GBzv2cE/UmbT/CJoRJ7PE2ND1HbnnhPUlkuH4z9e4ccrkIvRIz3oVk12d/8pQI/wvtX8hRBXBvoGnRAJQckuREJQsguREJTsQiQEJbsQCaGtDSfrnsJyNSy95EhFGQCMHx0Ojud3LQTHAaCS55VL1QKXk2wbr/KqL5AvBXXw4xXnuGTUPxyuAASArXluu6Gfb0H0TOGq4Pipu3mlX/lq/phtlsefWeEyT4NcWbnZSFPJSX4NZJciFXElXrZ3biIsb852cnnt2l2T1HZiZpDauo/wa+7kdi4Tl2vhyRpf5tLsh0Z+ERyPSYO6swuREJTsQiQEJbsQCUHJLkRCULILkRCU7EIkhLZKbxlrYCC3ErQdH+TSSmzvLUYtz4/XyHPZJdJ7EenecByNGV7J5R28Ii5GbM+50c5wA04AONcXrnor3s57CXzlxj+nticX+Z55Pzy8n9pY1d7yApfyykd4jJU8vy91j3FbvSNsq/TzS//VSNOlbI5X2EV6SmK+yB/3ylJYsts3ME19npq9Nji+WH+J+ujOLkRCULILkRCU7EIkBCW7EAlByS5EQmjranw2Vcf2znBPsEYPXyFPkWXO64fCvekA4JmzPdTWM8j7sV01wHewOj0fLkwoneIrrTVwVWBurJfaVrbwFf6ncvuo7flTu4LjHhE7/vvUO6ltKMcLcmolfvmUU+ETdhd40c30LXweO+aoCY1I02Inokaat89D+iQvaKn18OclUg+FRoMXqPT3LwfHR7tnqE8+HZ7H/5viypXu7EIkBCW7EAlByS5EQlCyC5EQlOxCJAQluxAJYVXpzcx2A/gmmlsyO4AH3f2rZvYFAJ8AcP7b+p9z9x/HjlVtpHGmVAjaUku88IOpFmeWuXSVneZ6THGRP+zDZ7hkZ/VwIPlpLqsUt/HX00i7MPTt4fLgl/Y8Qm2f8d8Jjh8e4z3oerN826WDU3uoDRX+2FL5sFxaq0V8IltvMQkNACJqE5Xsank++ZVeXtGSLnK/2DZgewe4dliqha/Vn0y+jfoMd4XluuX609RnLTp7DcDvufvPzawA4Gdm9njL9hV3/y9rOIYQYpNZy15vEwAmWj8vmtlhADs3OjAhxOXloj6zm9kogFsBHGwNfdrMnjezh8yM98oVQmw6a052M8sD+AGAz7r7AoCvAbgGwH407/xfIn73m9khMztUnuOfQ4UQG8uakt3Msmgm+rfc/YcA4O6T7l539waArwO4I+Tr7g+6+wF3P5Dr77pccQshLpJVk93MDMA3ABx29y9fMD5ywZ99CMCLlz88IcTlYi2r8e8C8DEAL5jZs62xzwH4qJntR1OOOw7gd1c7UM1TmCmFpa1UlUsawzvDlWgf2MFfX/5knG/TYykukeRO8Iontt1RpY8fL7PMH1cm8qlm+gzf+uebu4JvogAAJ2bDSye9hXDvPwDYnz9Jbdd2862mft4f3moKAHZ1hqWmgzOj1OdUd57aSsN8jj1SbsbkUu/hveQ687wkrjwR7vEHADbA/c4Vud/Z6bCE/A+uf5X6zFd4hSBjLavxPwUQmrGopi6EuLLQN+iESAhKdiESgpJdiISgZBciISjZhUgIbW04WaunMLUQlldqw7x0qVgJVwWdKnF5LYYXI40Sd3P5pLIQjiNd5vJabYA/ro4zkemPyIN9GS6j7dsS3jKoFtlOanuGV2RtSfOGk/s7uWT3UjlcPnHsOJcNeyf5PC7v5vNR74l0euwO2/K9vNLvjhH+uHqu5g0zezP8mMU6r8I8mB4Njv/RrseD4wDwn2duD44/HemkqTu7EAlByS5EQlCyC5EQlOxCJAQluxAJQckuREJoq/SWy9RwzVB4/6qZHl4VdPPgRHD87v4XqM/fbBmlNo90euzv5qVoJ14NN23sPsZfM2uzXHKpFrichEhjxhj7+04Hxx899Xbq86fj76G2OwfeoLY9ubPUliZ73P39m3gl1zMnb6a2WFVkqofLm5lsuItlqcj3bJut8L4LYyu8GvHaXr73YFc6EiPZF+8npX7qk7Nw1V6kh6nu7EIkBSW7EAlByS5EQlCyC5EQlOxCJAQluxAJoa3SW6Wexun5sHRRqfFQTuXCTRR/kr6e+hTLXFqpkyaEq5GbDFeOdc3wqqv6QmRvsAq3eYrPx9GVbdy2OBQcPzfP97A7N8cbPcb2iEtn+OZsIwMLwfGZZS6xxm49jciVWp/lTUJTQ2EptbbMJdHxJS6vMZkMAI4uDlPbtq5FatveQ+aqxp+X11a2BsfLkYnSnV2IhKBkFyIhKNmFSAhKdiESgpJdiISw6mq8mXUCeApArvX333f3z5vZ1QC+A2ALgJ8B+Ji78wZYAPLZCt6941jQ9vTUHup3fCbca+7Y9BbqUx3jq8+NTr6iulDlvdoaO8LFB+WtfFX9xptPUNtciRdcvKP3HLX98c6D1Pb7k/uD4/dse4n6/HyBb+M0tsyLMRZKfAuiuZXwY+vu4AUhZ3siWzxlua1zil/Gpa6wKnPD28aoT4xGpIgqtuJ+W4H3tbsuNx4c/3/L11KfiZXwllGVSK/BtdzZywDe6+63oLk98z1mdieAPwTwFXd/G4BZAB9fw7GEEJvEqsnuTc63GM22/jmA9wL4fmv8YQAf3JAIhRCXhbXuz55u7eA6BeBxAK8DmHP38+9rTwMI9w4WQlwRrCnZ3b3u7vsB7AJwBwD+1bW3YGb3m9khMztUnOV9tYUQG8tFrca7+xyAJwG8E0C/mZ1fGdkFILji4e4PuvsBdz/QNXDxe0oLIS4Pqya7mQ2bWX/r5y4AvwngMJpJ/zutP7sPwI82KkghxPpZSyHMCICHzSyN5ovD99z9L8zsZQDfMbP/AOAXAL6x2oFK9QwOz4cLK+aWuAzFiirmixHpZwsvdNixlW93FCvIWSYyTmmFF928Ms6LVhoNLuPs7eP93f5olsuUZ8vh4olrOnl/tHyGK6Y9WW67e9thanthMbyEM9I5T31e6+HSVc35fen4Ob4N2N/beiY4/t7BI9Tn9VK4yASIF5o0IjEOZ8LXMACseLiQ59ZuLtu+s+docPwzHfw8qya7uz8P4NbA+DE0P78LIX4J0DfohEgISnYhEoKSXYiEoGQXIiEo2YVICOYe2YLocp/MbBrAeT1hCADXl9qH4ngziuPN/LLFscfdg83w2prsbzqx2SF3P7ApJ1cciiOBcehtvBAJQckuRELYzGR/cBPPfSGK480ojjfzKxPHpn1mF0K0F72NFyIhbEqym9k9ZvaKmR01swc2I4ZWHMfN7AUze9bMDrXxvA+Z2ZSZvXjB2KCZPW5mr7X+D+95tfFxfMHMxlpz8qyZvb8Ncew2syfN7GUze8nMPtMab+ucROJo65yYWaeZPW1mz7Xi+Pet8avN7GArb75rZrzcMoS7t/UfgDSaba32AugA8ByAG9sdRyuW4wCGNuG87wFwG4AXLxj7TwAeaP38AIA/3KQ4vgDg37R5PkYA3Nb6uQDgVQA3tntOInG0dU4AGIB86+csgIMA7gTwPQAfaY3/CYBPXsxxN+POfgeAo+5+zJutp78D4N5NiGPTcPenALy1V/S9aDbuBNrUwJPE0XbcfcLdf976eRHN5ig70eY5icTRVrzJZW/yuhnJvhPAqQt+38xmlQ7gMTP7mZndv0kxnGebu0+0fj4DgHe92Hg+bWbPt97mb/jHiQsxs1E0+yccxCbOyVviANo8JxvR5DXpC3TvdvfbAPxjAJ8ys/dsdkBA85UdzReizeBrAK5Bc4+ACQBfateJzSwP4AcAPuvub2q50s45CcTR9jnxdTR5ZWxGso8B2H3B77RZ5Ubj7mOt/6cAPILN7bwzaWYjAND6n/eR2kDcfbJ1oTUAfB1tmhMzy6KZYN9y9x+2hts+J6E4NmtOWue+6CavjM1I9mcA7GutLHYA+AiAR9sdhJn1mFnh/M8A7gbwYtxrQ3kUzcadwCY28DyfXC0+hDbMiZkZmj0MD7v7ly8wtXVOWBztnpMNa/LarhXGt6w2vh/Nlc7XAfy7TYphL5pKwHMAXmpnHAC+jebbwSqan70+juaeeU8AeA3A/wEwuElx/DcALwB4Hs1kG2lDHO9G8y368wCebf17f7vnJBJHW+cEwDvQbOL6PJovLH9wwTX7NICjAP4ngNzFHFffoBMiISR9gU6IxKBkFyIhKNmFSAhKdiESgpJdiISgZBciISjZhUgISnYhEsL/B93WFdF+fxoXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
    "\n",
    "noise = tf.random.normal([1, SEED_SIZE])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlxxEHDIOqjW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1878,
     "status": "ok",
     "timestamp": 1602700165726,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "LOnTxIXnyeEQ",
    "outputId": "035cd1b1-f4d3-444d-aa1d-0ca64ee27a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.49933702]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
    "\n",
    "discriminator = build_discriminator(image_shape)\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ChOo3D1OsVc"
   },
   "source": [
    "Loss functions must be developed that allow the generator and discriminator to be trained in an adversarial way.  Because these two neural networks are being trained independently they must be trained in two separate passes.  This requires two separate loss functions and also two separate updates to the gradients.  When the discriminator's gradients are applied to decrease the discriminator's loss it is important that only the discriminator's weights are update.  It is not fair, nor will it produce good results, to adversarially damage the weights of the generator to help the discriminator.  A simple backpropagation would do this.  It would simultaneously affect the weights of both generator and discriminator to lower whatever loss it was assigned to lower.\n",
    "\n",
    "Figure 7.TDIS shows how the discriminator is trained.\n",
    "\n",
    "**Figure 7.TDIS: Training the Discriminator**\n",
    "![Training the Discriminator](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_2.png \"Training the Discriminator\")\n",
    "\n",
    "Here a training set is generated with an equal number of real and fake images.  The real images are randomly sampled (chosen) from the training data.  An equal number of random images are generated from random seeds.  For the discriminator training set, the $x$ contains the input images and the $y$ contains a value of 1 for real images and 0 for generated ones.\n",
    "\n",
    "Likewise, the Figure 7.TGEN shows how the generator is trained.\n",
    "\n",
    "**Figure 7.TGEN: Training the Generator**\n",
    "![Training the Generator](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_fig_3.png \"Training the Generator\")\n",
    "\n",
    "For the generator training set, the $x$ contains the random seeds to generate images and the $y$ always contains the value of 1, because the optimal is for the generator to have generated such good images that the discriminiator was fooled into assigning them a probability near 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 2505,
     "status": "ok",
     "timestamp": 1602700166358,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "gBaP98zAySJV"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIlZvHGAxbWf"
   },
   "source": [
    "Both the generator and discriminator use Adam and the same learning rate and momentum.  This does not need to be the case.  If you use a **GENERATE_RES** greater than 3 you may need to tune these learning rates, as well as other training and hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 2502,
     "status": "ok",
     "timestamp": 1602700166358,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "79UDhOCa0R4h"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frCpCNn8yRcM"
   },
   "source": [
    "The following function is where most of the training takes place for both the discriminator and the generator.  This function was based on the GAN provided by the [TensorFlow Keras exmples](https://www.tensorflow.org/tutorials/generative/dcgan) documentation.  The first thing you should notice about this function is that it is annotated with the **tf.function** annotation.  This causes the function to be precompiled and improves performance.\n",
    "\n",
    "This function trans differently than the code we previously saw for training.  This code makes use of **GradientTape** to allow the discriminator and generator to be trained together, yet separately.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1602700166360,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "uzyh-LqU0j5d"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(seed, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\\\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\\\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "  return gen_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 2495,
     "status": "ok",
     "timestamp": 1602700166360,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "NjrRgDR10lSF"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
    "                                       SEED_SIZE))\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    gen_loss_list = []\n",
    "    disc_loss_list = []\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      t = train_step(image_batch)\n",
    "      gen_loss_list.append(t[0])\n",
    "      disc_loss_list.append(t[1])\n",
    "\n",
    "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
    "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
    "\n",
    "    epoch_elapsed = time.time()-epoch_start\n",
    "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss},'\\\n",
    "           ' {hms_string(epoch_elapsed)}')\n",
    "    save_images(epoch,fixed_seed)\n",
    "\n",
    "  elapsed = time.time()-start\n",
    "  print (f'Training time: {hms_string(elapsed)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4661997,
     "status": "ok",
     "timestamp": 1602704825867,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "vWmEHprD0t1V",
    "outputId": "ce337f22-c74d-432d-d558-2007fe80d7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen loss=0.655738115310669,disc loss=1.2579939365386963, {hms_string(epoch_elapsed)}\n",
      "Epoch 2, gen loss=0.6609563827514648,disc loss=1.303942084312439, {hms_string(epoch_elapsed)}\n",
      "Epoch 3, gen loss=0.6571552157402039,disc loss=1.3204377889633179, {hms_string(epoch_elapsed)}\n",
      "Epoch 4, gen loss=0.6576387286186218,disc loss=1.3262916803359985, {hms_string(epoch_elapsed)}\n",
      "Epoch 5, gen loss=0.6592935919761658,disc loss=1.332157015800476, {hms_string(epoch_elapsed)}\n",
      "Epoch 6, gen loss=0.6569798588752747,disc loss=1.3292055130004883, {hms_string(epoch_elapsed)}\n",
      "Epoch 7, gen loss=0.6570515036582947,disc loss=1.3317688703536987, {hms_string(epoch_elapsed)}\n",
      "Epoch 8, gen loss=0.6578193306922913,disc loss=1.3333475589752197, {hms_string(epoch_elapsed)}\n",
      "Epoch 9, gen loss=0.659172534942627,disc loss=1.3344911336898804, {hms_string(epoch_elapsed)}\n",
      "Epoch 10, gen loss=0.6587169170379639,disc loss=1.3282479047775269, {hms_string(epoch_elapsed)}\n",
      "Epoch 11, gen loss=0.6562275290489197,disc loss=1.336403727531433, {hms_string(epoch_elapsed)}\n",
      "Epoch 12, gen loss=0.6583821177482605,disc loss=1.3267810344696045, {hms_string(epoch_elapsed)}\n",
      "Epoch 13, gen loss=0.6575711965560913,disc loss=1.329576849937439, {hms_string(epoch_elapsed)}\n",
      "Epoch 14, gen loss=0.6579015851020813,disc loss=1.3216804265975952, {hms_string(epoch_elapsed)}\n",
      "Epoch 15, gen loss=0.6564159393310547,disc loss=1.320548176765442, {hms_string(epoch_elapsed)}\n",
      "Epoch 16, gen loss=0.6561840176582336,disc loss=1.311208724975586, {hms_string(epoch_elapsed)}\n",
      "Epoch 17, gen loss=0.657298743724823,disc loss=1.3168509006500244, {hms_string(epoch_elapsed)}\n",
      "Epoch 18, gen loss=0.655549943447113,disc loss=1.315246820449829, {hms_string(epoch_elapsed)}\n",
      "Epoch 19, gen loss=0.6573776006698608,disc loss=1.3081390857696533, {hms_string(epoch_elapsed)}\n",
      "Epoch 20, gen loss=0.656762421131134,disc loss=1.306222677230835, {hms_string(epoch_elapsed)}\n",
      "Epoch 21, gen loss=0.6572110652923584,disc loss=1.3046048879623413, {hms_string(epoch_elapsed)}\n",
      "Epoch 22, gen loss=0.6581577658653259,disc loss=1.3070118427276611, {hms_string(epoch_elapsed)}\n",
      "Epoch 23, gen loss=0.6573815941810608,disc loss=1.2968101501464844, {hms_string(epoch_elapsed)}\n",
      "Epoch 24, gen loss=0.6591730713844299,disc loss=1.29326331615448, {hms_string(epoch_elapsed)}\n",
      "Epoch 25, gen loss=0.6579867601394653,disc loss=1.2940694093704224, {hms_string(epoch_elapsed)}\n",
      "Epoch 26, gen loss=0.6567611694335938,disc loss=1.2920057773590088, {hms_string(epoch_elapsed)}\n",
      "Epoch 27, gen loss=0.6583637595176697,disc loss=1.28518545627594, {hms_string(epoch_elapsed)}\n",
      "Epoch 28, gen loss=0.6621875762939453,disc loss=1.2711278200149536, {hms_string(epoch_elapsed)}\n",
      "Epoch 29, gen loss=0.6613527536392212,disc loss=1.2716805934906006, {hms_string(epoch_elapsed)}\n",
      "Epoch 30, gen loss=0.6599454879760742,disc loss=1.2812052965164185, {hms_string(epoch_elapsed)}\n",
      "Epoch 31, gen loss=0.6604519486427307,disc loss=1.2746752500534058, {hms_string(epoch_elapsed)}\n",
      "Epoch 32, gen loss=0.6636804342269897,disc loss=1.2679694890975952, {hms_string(epoch_elapsed)}\n",
      "Epoch 33, gen loss=0.6631941199302673,disc loss=1.2641948461532593, {hms_string(epoch_elapsed)}\n",
      "Epoch 34, gen loss=0.6644511222839355,disc loss=1.2604607343673706, {hms_string(epoch_elapsed)}\n",
      "Epoch 35, gen loss=0.6625658273696899,disc loss=1.261590838432312, {hms_string(epoch_elapsed)}\n",
      "Epoch 36, gen loss=0.6625891327857971,disc loss=1.2620277404785156, {hms_string(epoch_elapsed)}\n",
      "Epoch 37, gen loss=0.666299045085907,disc loss=1.2532256841659546, {hms_string(epoch_elapsed)}\n",
      "Epoch 38, gen loss=0.6751970648765564,disc loss=1.2270097732543945, {hms_string(epoch_elapsed)}\n",
      "Epoch 39, gen loss=0.6714730858802795,disc loss=1.2290931940078735, {hms_string(epoch_elapsed)}\n",
      "Epoch 40, gen loss=0.6686282753944397,disc loss=1.208105206489563, {hms_string(epoch_elapsed)}\n",
      "Epoch 41, gen loss=0.6624535918235779,disc loss=1.1547101736068726, {hms_string(epoch_elapsed)}\n",
      "Epoch 42, gen loss=0.6600630283355713,disc loss=1.1204861402511597, {hms_string(epoch_elapsed)}\n",
      "Epoch 43, gen loss=0.6744420528411865,disc loss=1.0675621032714844, {hms_string(epoch_elapsed)}\n",
      "Epoch 44, gen loss=0.6877960562705994,disc loss=1.0349531173706055, {hms_string(epoch_elapsed)}\n",
      "Epoch 45, gen loss=0.6930865049362183,disc loss=1.0065324306488037, {hms_string(epoch_elapsed)}\n",
      "Epoch 46, gen loss=0.6872686743736267,disc loss=1.039280891418457, {hms_string(epoch_elapsed)}\n",
      "Epoch 47, gen loss=0.6924958229064941,disc loss=1.0083403587341309, {hms_string(epoch_elapsed)}\n",
      "Epoch 48, gen loss=0.6931456923484802,disc loss=1.00641667842865, {hms_string(epoch_elapsed)}\n",
      "Epoch 49, gen loss=0.6931442618370056,disc loss=1.006414532661438, {hms_string(epoch_elapsed)}\n",
      "Epoch 50, gen loss=0.6931458115577698,disc loss=1.006412148475647, {hms_string(epoch_elapsed)}\n",
      "Epoch 51, gen loss=0.6931456923484802,disc loss=1.006410837173462, {hms_string(epoch_elapsed)}\n",
      "Epoch 52, gen loss=0.6931442618370056,disc loss=1.0064127445220947, {hms_string(epoch_elapsed)}\n",
      "Epoch 53, gen loss=0.6931450963020325,disc loss=1.0064140558242798, {hms_string(epoch_elapsed)}\n",
      "Epoch 54, gen loss=0.6931450963020325,disc loss=1.006410837173462, {hms_string(epoch_elapsed)}\n",
      "Epoch 55, gen loss=0.6931453347206116,disc loss=1.0064101219177246, {hms_string(epoch_elapsed)}\n",
      "Epoch 56, gen loss=0.693143367767334,disc loss=1.0064116716384888, {hms_string(epoch_elapsed)}\n",
      "Epoch 57, gen loss=0.6931456327438354,disc loss=1.0064101219177246, {hms_string(epoch_elapsed)}\n",
      "Epoch 58, gen loss=0.6931460499763489,disc loss=1.0064094066619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 59, gen loss=0.6931460499763489,disc loss=1.0064091682434082, {hms_string(epoch_elapsed)}\n",
      "Epoch 60, gen loss=0.6931461095809937,disc loss=1.0064094066619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 61, gen loss=0.6931459307670593,disc loss=1.0064092874526978, {hms_string(epoch_elapsed)}\n",
      "Epoch 62, gen loss=0.6931393146514893,disc loss=1.0064209699630737, {hms_string(epoch_elapsed)}\n",
      "Epoch 63, gen loss=0.6931459903717041,disc loss=1.0064090490341187, {hms_string(epoch_elapsed)}\n",
      "Epoch 64, gen loss=0.6931458711624146,disc loss=1.0064092874526978, {hms_string(epoch_elapsed)}\n",
      "Epoch 65, gen loss=0.6931461095809937,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 66, gen loss=0.6931461095809937,disc loss=1.0064094066619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 67, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 68, gen loss=0.6931459903717041,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 69, gen loss=0.6931459903717041,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 70, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 71, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 72, gen loss=0.6931459307670593,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 73, gen loss=0.6931460499763489,disc loss=1.0064090490341187, {hms_string(epoch_elapsed)}\n",
      "Epoch 74, gen loss=0.6931459903717041,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 75, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 76, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 77, gen loss=0.6931459903717041,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 78, gen loss=0.6931460499763489,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 79, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 80, gen loss=0.6915277242660522,disc loss=1.0160045623779297, {hms_string(epoch_elapsed)}\n",
      "Epoch 81, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 82, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 83, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 84, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 85, gen loss=0.6931459307670593,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 86, gen loss=0.6931440234184265,disc loss=1.006410837173462, {hms_string(epoch_elapsed)}\n",
      "Epoch 87, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 88, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 89, gen loss=0.6931461095809937,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 90, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 91, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 92, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 93, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 94, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 95, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 96, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 97, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 98, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 99, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 100, gen loss=0.6931453943252563,disc loss=1.0064113140106201, {hms_string(epoch_elapsed)}\n",
      "Epoch 101, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 102, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 103, gen loss=0.6931460499763489,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 104, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 105, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 106, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 107, gen loss=0.6851274967193604,disc loss=1.0546149015426636, {hms_string(epoch_elapsed)}\n",
      "Epoch 108, gen loss=0.692255973815918,disc loss=1.0098642110824585, {hms_string(epoch_elapsed)}\n",
      "Epoch 109, gen loss=0.6931460499763489,disc loss=1.0064094066619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 110, gen loss=0.6931461095809937,disc loss=1.0064094066619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 111, gen loss=0.6924817562103271,disc loss=1.0090916156768799, {hms_string(epoch_elapsed)}\n",
      "Epoch 112, gen loss=0.6931461095809937,disc loss=1.0065486431121826, {hms_string(epoch_elapsed)}\n",
      "Epoch 113, gen loss=0.6931461095809937,disc loss=1.006408929824829, {hms_string(epoch_elapsed)}\n",
      "Epoch 114, gen loss=0.6931461095809937,disc loss=1.0064098834991455, {hms_string(epoch_elapsed)}\n",
      "Epoch 115, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 116, gen loss=0.6931461095809937,disc loss=1.0064091682434082, {hms_string(epoch_elapsed)}\n",
      "Epoch 117, gen loss=0.6931461095809937,disc loss=1.0064090490341187, {hms_string(epoch_elapsed)}\n",
      "Epoch 118, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 119, gen loss=0.6931461095809937,disc loss=1.0064096450805664, {hms_string(epoch_elapsed)}\n",
      "Epoch 120, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 121, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 122, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 123, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 124, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 125, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 126, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 127, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 128, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 129, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 130, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 131, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 132, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 133, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 134, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 135, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 136, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 137, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 138, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 139, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 140, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 141, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 142, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 143, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 144, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 145, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 146, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 147, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 148, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 149, gen loss=0.6931461095809937,disc loss=1.0064088106155396, {hms_string(epoch_elapsed)}\n",
      "Epoch 150, gen loss=0.6889753341674805,disc loss=1.0267583131790161, {hms_string(epoch_elapsed)}\n",
      "Epoch 151, gen loss=0.6920803189277649,disc loss=1.0097739696502686, {hms_string(epoch_elapsed)}\n",
      "Epoch 152, gen loss=0.6824984550476074,disc loss=1.0896254777908325, {hms_string(epoch_elapsed)}\n",
      "Epoch 153, gen loss=0.6912288665771484,disc loss=1.1500486135482788, {hms_string(epoch_elapsed)}\n",
      "Epoch 154, gen loss=0.6931456923484802,disc loss=1.0064160823822021, {hms_string(epoch_elapsed)}\n",
      "Epoch 155, gen loss=0.6908764243125916,disc loss=1.1823046207427979, {hms_string(epoch_elapsed)}\n",
      "Epoch 156, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 157, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 158, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 159, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 160, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 161, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 162, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 163, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 164, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 165, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 166, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 167, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 168, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 169, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 170, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 171, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 172, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 173, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 174, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 175, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 176, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 177, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 178, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 179, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 180, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 181, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 182, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 183, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 184, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 185, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 186, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 187, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 188, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 189, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 190, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 191, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 192, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 193, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 194, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 195, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 196, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 197, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 198, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 199, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 200, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 201, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 202, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 203, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 204, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 205, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 206, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 207, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 208, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 209, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 210, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 211, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 212, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 213, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 214, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 215, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 216, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 217, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 218, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 219, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 220, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 221, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 222, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 223, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 224, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 225, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 226, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 227, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 228, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 229, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 230, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 231, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 232, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 233, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 234, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 235, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 236, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 237, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 238, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 239, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 240, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 241, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 242, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 243, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 244, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 245, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 246, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 247, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 248, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 249, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 250, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 251, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 252, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 253, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 254, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 255, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 256, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 257, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 258, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 259, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 260, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 261, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 262, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 263, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 264, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 265, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 266, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 267, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 268, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 269, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 270, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 271, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 272, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 273, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 274, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 275, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 276, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 277, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 278, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 279, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 280, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 281, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 282, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 283, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 284, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 285, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 286, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 287, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 288, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 289, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 290, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 291, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 292, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 293, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 294, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 295, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 296, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 297, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 298, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 299, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 300, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 301, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 302, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 303, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 304, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 305, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 306, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 307, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 308, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 309, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 310, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 311, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 312, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 313, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 314, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 315, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 316, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 317, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 318, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 319, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 320, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 321, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 322, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 323, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 324, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 325, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 326, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 327, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 328, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 329, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 330, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 331, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 332, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 333, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 334, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 335, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 336, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 337, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 338, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 339, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 340, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 341, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 342, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 343, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 344, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 345, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 346, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 347, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 348, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 349, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 350, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 351, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 352, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 353, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 354, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 355, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 356, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 357, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 358, gen loss=0.6833345293998718,disc loss=1.2629594802856445, {hms_string(epoch_elapsed)}\n",
      "Epoch 359, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 360, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 361, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 362, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 363, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 364, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 365, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 366, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 367, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 368, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 369, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 370, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 371, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 372, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 373, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 374, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 375, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 376, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 377, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 378, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 379, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 380, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 381, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 382, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 383, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 384, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 385, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 386, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 387, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 388, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 389, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 390, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 391, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 392, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 393, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 394, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 395, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 396, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 397, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 398, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 399, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 400, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 401, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 402, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 403, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 404, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 405, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 406, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 407, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 408, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 409, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 410, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 411, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 412, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 413, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 414, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 415, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 416, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 417, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 418, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 419, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 420, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 421, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 422, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 423, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 424, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 425, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 426, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 427, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 428, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 429, gen loss=0.6930257678031921,disc loss=1.3850339651107788, {hms_string(epoch_elapsed)}\n",
      "Epoch 430, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 431, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 432, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 433, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 434, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 435, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 436, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 437, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 438, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 439, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 440, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 441, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 442, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 443, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 444, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 445, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 446, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 447, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 448, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 449, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 450, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 451, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 452, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 453, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 454, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 455, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 456, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 457, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 458, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 459, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 460, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 461, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 462, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 463, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 464, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 465, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 466, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 467, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 468, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 469, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 470, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 471, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 472, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 473, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 474, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 475, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 476, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 477, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 478, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 479, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 480, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 481, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 482, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 483, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 484, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 485, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 486, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 487, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 488, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 489, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 490, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 491, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 492, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 493, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 494, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 495, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 496, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 497, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 498, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 499, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Epoch 500, gen loss=0.6931461095809937,disc loss=1.3862922191619873, {hms_string(epoch_elapsed)}\n",
      "Training time: 1:17:39.35\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 4661999,
     "status": "ok",
     "timestamp": 1602704825873,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "15Hia_feD9sm"
   },
   "outputs": [],
   "source": [
    "#Model_PATH  = '/content/drive/My Drive/projects'\n",
    "#generator.save(os.path.join(Model_PATH ,\"face_generator8by8.h5\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Keras_gan.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_Keras_gan.ipynb",
     "timestamp": 1602515072394
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
