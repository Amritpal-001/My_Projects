{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3.6 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Style_gan.ipynb","provenance":[{"file_id":"https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb","timestamp":1602511770601}],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"L2fejML3tCfv"},"source":["<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"4Nv0VgiBJYDP"},"source":["Original colab notebook - \n","\n","https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb#scrollTo=7sUD8c5JCfkg"]},{"cell_type":"markdown","metadata":{"id":"nTYnTqOPtCfy"},"source":["# T81-558: Applications of Deep Neural Networks\n","**Module 7: Generative Adversarial Networks**\n","* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n","* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."]},{"cell_type":"markdown","metadata":{"id":"oZIf2h_itCf0"},"source":["# Module 7 Material\n","\n","* Part 7.1: Introduction to GANS for Image and Data Generation [[Video]](https://www.youtube.com/watch?v=0QnCH6tlZgc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_1_gan_intro.ipynb)\n","* Part 7.2: Implementing a GAN in Keras [[Video]](https://www.youtube.com/watch?v=T-MCludVNn4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_2_Keras_gan.ipynb)\n","* **Part 7.3: Face Generation with StyleGAN and Python** [[Video]](https://www.youtube.com/watch?v=Wwwyr7cOBlU&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_3_style_gan.ipynb)\n","* Part 7.4: GANS for Semi-Supervised Learning in Keras [[Video]](https://www.youtube.com/watch?v=ZPewmEu7644&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_4_gan_semi_supervised.ipynb)\n","* Part 7.5: An Overview of GAN Research [[Video]](https://www.youtube.com/watch?v=cvCvZKvlvq4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_07_5_gan_research.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"IIRFv-IHtCf1"},"source":["## Warning: This Module Requires TensorFlow 1.x\n","\n","This module makes use of the nVidia StyleGAN2 package.  Neither StyleGAN 1.0 nor StyleGAN 2.0 currently supports TensorFlow 2.0.  Because of this incompatibility, it will be necessary to run this code with an older version of TensorFlow.  Running this notebook in this way is easiest with Google CoLab.  Because of this, I designed this module to run in Google CoLab.  It will take some modifications to if you wish to run it locally.\n","\n","Also, note that this module uses StyleGAN 1.0, I will soon update to 2.0."]},{"cell_type":"markdown","metadata":{"id":"4xVz_59FtCf3"},"source":["# Part 7.3: Face Generation with StyleGAN and Python\n","\n","GANs have appeared frequently in the media, showcasing their ability to generate extremely photorealistic faces.  One significant step forward for realistic face generation was nVidia StyleGAN. [[Cite:karras2019style]](https://arxiv.org/abs/1812.04948) In this module we will explore StyleGAN2, which is the second interation of this technology by nVidia. [[Cite:karras2019analyzing]](https://arxiv.org/abs/1912.04958) We will also preload weights that nVidia trained on.  This will allow us to generate high resolution photorealistic looking faces, such seen in Figure 7.STY-GAN.\n","\n","**Figure 7.STY-GAN: StyleGAN2 Generated Faces**\n","![StyleGAN2 Generated Faces](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2_images.jpg \"StyleGAN2 Generated Faces\")\n","\n","The above images were generated with StyleGAN2, using Google CoLab.  Following the instructions in this section, you will be able to create faces like this of your own.  StyleGAN2 images are usually 1,024 x 1,024 in resolution.  An example of a full resolution StyleGAN image can be [found here](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/stylegan2-hires.jpg). \n","\n","While the above images look much more realistic than images generated earlier in this course, they are not perfect.  Look at Figure 7.STYLEGAN2. There are usually a number of tell-tail signs that you are looking at a computer generated image.  One of the most obvious is usually the surreal, dream-like backgrounds.  The background does not look obviously fake, at first glance; however, upon closer inspection you usually can't quite discern exactly what a GAN generated background actually is.  Also look at the image character's left eye.  It is slightly unrealistic looking, especially near the eyelashes.\n","\n","Look at the following GAN face.  Can you spot any imperfections?\n","\n","**Figure 7.STYLEGAN2: StyleGAN2 Face**\n","![StyleGAN2 Face](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_bad.jpg \"StyleGAN2 Face\")\n","\n","* Image A demonstrates the very abstract backgrounds usually associated with a GAN generated image.\n","* Image B exhibits issues that earrings often present for GANs. GANs sometimes have problems with symmetry, particularly earrings.\n","* Image C contains an abstract background, as well as a highly distorted secondary image.\n","* Image D also contains a highly distorted secondary image that might be a hand.\n","\n","There are a number of websites that allow you to generate GANs of your own without any software.\n","\n","* [This Person Does not Exist](https://www.thispersondoesnotexist.com/)\n","* [Which Face is Real](http://www.whichfaceisreal.com/)\n","\n","The first site generates high resolution images of human faces.  The second site presents a quiz to see if you can detect the difference between a real and fake human faceimage.\n","\n","In this module you will learn to create your own StyleGAN2 pictures using Python."]},{"cell_type":"markdown","metadata":{"id":"p44L5mC-tCf4"},"source":["### Keras Sequence vs Functional Model API\n","\n","Most of the neural networks create in this course have made use of the Keras sequence object.  You might have noticed that we briefly made use of another type of neural network object for the ResNet, the Model.  These are the [two major means](https://keras.io/getting-started/functional-api-guide/) of constructing a neural network in Keras:\n","\n","* [Sequential](https://keras.io/getting-started/sequential-model-guide/) - Simplified interface to Keras that supports most models where the flow of information is a simple sequence from input to output. \n","* [Keras Functional API](https://keras.io/getting-started/functional-api-guide/) - More complex interface that allows neural networks to be constructed of reused layers, multiple input layers, and supports building your own recurrent connections.\n","\n","It is important to point out that these are not two specific types of neural network.  Rather, they are two means of constructing neural networks in Keras.  Some types of neural network can be implemented in either, such as dense feedforward neural networks (like we used for the Iris and MPG datasets).  However, other types of neural network, like ResNet and GANs can only be used in the Functional Model API."]},{"cell_type":"markdown","metadata":{"id":"IWl5ywCjtCf5"},"source":["### Generating High Rez GAN Faces with Google CoLab\n","\n","This notebook demonstrates how to run [NVidia StyleGAN2](https://github.com/NVlabs/stylegan) inside of a Google CoLab notebook.  I suggest you use this to generate GAN faces from a pretrained model.  If you try to train your own, you will run into compute limitations of Google CoLab.\n","\n","Make sure to run this code on a GPU instance.  GPU is assumed.\n","\n","First, map your G-Drive, this is where your GANs will be written to."]},{"cell_type":"code","metadata":{"id":"w2dEcHb9tCf6","executionInfo":{"status":"ok","timestamp":1602510200951,"user_tz":-330,"elapsed":27328,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"e12c1255-930e-4fb6-90f5-18962ec94c97","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Run this for Google CoLab (use TensorFlow 1.x)\n","%tensorflow_version 1.x\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"icRPLmPKtCf_"},"source":["Next, clone StyleGAN2 from GitHub."]},{"cell_type":"code","metadata":{"id":"tB0TryzptCf_","executionInfo":{"status":"ok","timestamp":1602510203697,"user_tz":-330,"elapsed":28787,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"e7feab08-f30f-451a-a1a4-a642870b7a30","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["!git clone https://github.com/NVlabs/stylegan2.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'stylegan2'...\n","remote: Enumerating objects: 122, done.\u001b[K\n","remote: Total 122 (delta 0), reused 0 (delta 0), pack-reused 122\u001b[K\n","Receiving objects: 100% (122/122), 590.23 KiB | 799.00 KiB/s, done.\n","Resolving deltas: 100% (55/55), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s1Sh0xmOtCgC"},"source":["Verify that StyleGAN has been cloned."]},{"cell_type":"code","metadata":{"id":"wewBEme5tCgD","executionInfo":{"status":"ok","timestamp":1602510205166,"user_tz":-330,"elapsed":980,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"de5bb6e5-02ad-46a4-9ebe-e8bb0d17da42","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["!ls /content/stylegan2/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dataset_tool.py  LICENSE.txt\t\t README.md\t   run_training.py\n","dnnlib\t\t metrics\t\t run_generator.py  test_nvcc.cu\n","Dockerfile\t pretrained_networks.py  run_metrics.py    training\n","docs\t\t projector.py\t\t run_projector.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4WrIU6F3Iwe"},"source":["# Run StyleGan2 From Command Line"]},{"cell_type":"markdown","metadata":{"id":"GQGFXRI0tCgG"},"source":["Add the StyleGAN folder to Python so that you can import it.  The code below is based on code from NVidia. This actually generates your images."]},{"cell_type":"code","metadata":{"id":"Wn_lpC_p-4ag","executionInfo":{"status":"ok","timestamp":1602510301706,"user_tz":-330,"elapsed":94170,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"65e2a144-6098-4f8c-f502-2a5dd996cada","colab":{"base_uri":"https://localhost:8080/","height":610}},"source":["!python /content/stylegan2/run_generator.py generate-images \\\n","    --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n","  --seeds=6600-6625 --truncation-psi=0.5"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Local submit - run_dir: results/00000-generate-images\n","dnnlib: Running run_generator.generate_images() on localhost...\n","Loading networks from \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"...\n","Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl ... done\n","Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Compiling... Loading... Done.\n","Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Compiling... Loading... Done.\n","Generating image for seed 6600 (0/26) ...\n","Generating image for seed 6601 (1/26) ...\n","Generating image for seed 6602 (2/26) ...\n","Generating image for seed 6603 (3/26) ...\n","Generating image for seed 6604 (4/26) ...\n","Generating image for seed 6605 (5/26) ...\n","Generating image for seed 6606 (6/26) ...\n","Generating image for seed 6607 (7/26) ...\n","Generating image for seed 6608 (8/26) ...\n","Generating image for seed 6609 (9/26) ...\n","Generating image for seed 6610 (10/26) ...\n","Generating image for seed 6611 (11/26) ...\n","Generating image for seed 6612 (12/26) ...\n","Generating image for seed 6613 (13/26) ...\n","Generating image for seed 6614 (14/26) ...\n","Generating image for seed 6615 (15/26) ...\n","Generating image for seed 6616 (16/26) ...\n","Generating image for seed 6617 (17/26) ...\n","Generating image for seed 6618 (18/26) ...\n","Generating image for seed 6619 (19/26) ...\n","Generating image for seed 6620 (20/26) ...\n","Generating image for seed 6621 (21/26) ...\n","Generating image for seed 6622 (22/26) ...\n","Generating image for seed 6623 (23/26) ...\n","Generating image for seed 6624 (24/26) ...\n","Generating image for seed 6625 (25/26) ...\n","dnnlib: Finished run_generator.generate_images() in 1m 26s.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UnrUF5L1_3UI","executionInfo":{"status":"ok","timestamp":1602510310214,"user_tz":-330,"elapsed":1388,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"45efab93-1dc0-4d4c-8cc4-1aed7b921eac","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["!ls /content/results/00000-generate-images"],"execution_count":null,"outputs":[{"output_type":"stream","text":["_finished.txt  seed6604.png  seed6611.png  seed6618.png  seed6625.png\n","log.txt        seed6605.png  seed6612.png  seed6619.png  submit_config.pkl\n","run.txt        seed6606.png  seed6613.png  seed6620.png  submit_config.txt\n","seed6600.png   seed6607.png  seed6614.png  seed6621.png\n","seed6601.png   seed6608.png  seed6615.png  seed6622.png\n","seed6602.png   seed6609.png  seed6616.png  seed6623.png\n","seed6603.png   seed6610.png  seed6617.png  seed6624.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3rh5faG6AOFA"},"source":["cp /content/results/00000-generate-images/*.png '/content/drive/My Drive/StyleGAN'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9YXGHyRE7aH","executionInfo":{"status":"ok","timestamp":1602510491548,"user_tz":-330,"elapsed":49835,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"56f16c9b-9310-4ac5-d8d2-550223bbd836","colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["!python /content/stylegan2/run_generator.py style-mixing-example \\\n","    --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \\\n","  --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 \\\n","    --truncation-psi=1.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Local submit - run_dir: results/00001-style-mixing-example\n","dnnlib: Running run_generator.style_mixing_example() on localhost...\n","Loading networks from \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"...\n","Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n","Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n","Generating W vectors...\n","Generating images...\n","Generating style-mixed images...\n","Saving images...\n","Saving image grid...\n","dnnlib: Finished run_generator.style_mixing_example() in 46s.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D92Md-Hw3eVA"},"source":["# Run StyleGAN2 From Python Code\n","\n","Add the StyleGAN folder to Python so that you can import it.  The code below is based on code from NVidia. This actually generates your images."]},{"cell_type":"code","metadata":{"id":"UgMm1sSutCgH"},"source":["import sys\n","sys.path.insert(0, \"/content/stylegan2\")\n","\n","import dnnlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZcFAjaz3mw1","executionInfo":{"status":"ok","timestamp":1602510641819,"user_tz":-330,"elapsed":16498,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5c93243b-e807-4015-e8f8-792849b09061","colab":{"base_uri":"https://localhost:8080/","height":382}},"source":["# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n","#\n","# This work is made available under the Nvidia Source Code License-NC.\n","# To view a copy of this license, visit\n","# https://nvlabs.github.io/stylegan2/license.html\n","\n","import argparse\n","import numpy as np\n","import PIL.Image\n","import dnnlib\n","import dnnlib.tflib as tflib\n","import re\n","import sys\n","\n","import pretrained_networks\n","\n","#----------------------------------------------------------------------------\n","\n","def expand_seed(seeds, vector_size):\n","  result = []\n","\n","  for seed in seeds:\n","    rnd = np.random.RandomState(seed)\n","    result.append( rnd.randn(1, vector_size) ) \n","  return result\n","\n","def generate_images(Gs, seeds, truncation_psi):\n","    noise_vars = [var for name, var in \\\n","                  Gs.components.synthesis.vars.items() \\\n","                  if name.startswith('noise')]\n","\n","    Gs_kwargs = dnnlib.EasyDict()\n","    Gs_kwargs.output_transform = dict(func= \\\n","        tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n","    Gs_kwargs.randomize_noise = False\n","    if truncation_psi is not None:\n","        Gs_kwargs.truncation_psi = truncation_psi\n","\n","    for seed_idx, seed in enumerate(seeds):\n","        print('Generating image for seed %d/%d ...' % (seed_idx, len(seeds)))\n","        rnd = np.random.RandomState()\n","        tflib.set_vars({var: rnd.randn(*var.shape.as_list()) \\\n","                        for var in noise_vars}) # [height, width]\n","        images = Gs.run(seed, None, **Gs_kwargs) \n","        # [minibatch, height, width, channel]\n","        path = f\"/content/drive/My Drive/\"+\\\n","                f\"StyleGAN/image{seed_idx}.png\"\n","        PIL.Image.fromarray(images[0], 'RGB').save(path)\n","\n","def main():\n","    sc = dnnlib.SubmitConfig()\n","    sc.num_gpus = 1\n","    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n","    sc.local.do_not_copy_source_files = True\n","    sc.run_dir_root = \"/content/drive/My Drive/StyleGAN\"\n","    sc.run_desc = 'generate-images'\n","    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n","\n","    print('Loading networks from \"%s\"...' % network_pkl)\n","    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n","    vector_size = Gs.input_shape[1:][0]\n","    seeds = expand_seed( range(8000,8020), vector_size)\n","    generate_images(Gs, seeds,truncation_psi=0.5)\n","\n","#----------------------------------------------------------------------------\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","#----------------------------------------------------------------------------"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading networks from \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"...\n","Generating image for seed 0/20 ...\n","Generating image for seed 1/20 ...\n","Generating image for seed 2/20 ...\n","Generating image for seed 3/20 ...\n","Generating image for seed 4/20 ...\n","Generating image for seed 5/20 ...\n","Generating image for seed 6/20 ...\n","Generating image for seed 7/20 ...\n","Generating image for seed 8/20 ...\n","Generating image for seed 9/20 ...\n","Generating image for seed 10/20 ...\n","Generating image for seed 11/20 ...\n","Generating image for seed 12/20 ...\n","Generating image for seed 13/20 ...\n","Generating image for seed 14/20 ...\n","Generating image for seed 15/20 ...\n","Generating image for seed 16/20 ...\n","Generating image for seed 17/20 ...\n","Generating image for seed 18/20 ...\n","Generating image for seed 19/20 ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tYxNRMBIKeOP"},"source":["## Examining the Latent Vector\n","\n","Figure 7.LVEC shows the effects of transforming the latent vector between two images.\n","\n","**Figure 7.LVEC: Transforming the Latent Vector**\n","![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_progression.jpg \"GAN\")"]},{"cell_type":"code","metadata":{"id":"8jJ8prvsy3am","executionInfo":{"status":"ok","timestamp":1602511528399,"user_tz":-330,"elapsed":2739,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4936e6bd-0d79-4b5d-cec4-b93344a7667f","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["sc = dnnlib.SubmitConfig()\n","sc.num_gpus = 1\n","sc.submit_target = dnnlib.SubmitTarget.LOCAL\n","sc.local.do_not_copy_source_files = True\n","sc.run_dir_root = \"/content/drive/My Drive/StyleGAN\"\n","sc.run_desc = 'generate-images'\n","network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n","\n","print('Loading networks from \"%s\"...' % network_pkl)\n","_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n","vector_size = Gs.input_shape[1:][0]\n","# range(8192,8300)\n","seeds = expand_seed( [8192+1,8192+9], vector_size)\n","#generate_images(Gs, seeds,truncation_psi=0.5)\n","print(seeds[0].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading networks from \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"...\n","(1, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7sUD8c5JCfkg"},"source":["# 8192+1,8192+9\n","\n","STEPS = 300\n","diff = seeds[1] - seeds[0]\n","step = diff / STEPS\n","current = seeds[0].copy()\n","\n","seeds2 = []\n","for i in range(STEPS):\n","  seeds2.append(current)\n","  current = current + step\n","\n","generate_images(Gs, seeds2,truncation_psi=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fC8Sl-B9cx2"},"source":["You might wish to view these generate images as a video file.  The command line utility **ffmpeg** can be used to combine these images into a video.  The following command could be used to create this video.  You will usually have to install **ffmpeg** for the particular operating system that you are running.  Refer to the [ffmpeg](https://ffmpeg.org/) website for more details on installation.\n","```\n","ffmpeg -r 30 -i image%d.png -vcodec mpeg4 -y movie.mp4\n","```"]},{"cell_type":"code","metadata":{"id":"J49RvS5r4TbI","executionInfo":{"status":"ok","timestamp":1602517234557,"user_tz":-330,"elapsed":74291,"user":{"displayName":"Amrit Pal Singh","photoUrl":"","userId":"02634295327518435662"}},"outputId":"10b3b58a-9a7a-456b-9c8b-1900ddbd9627","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["!git clone https://git.ffmpeg.org/ffmpeg.git ffmpeg"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'ffmpeg'...\n","remote: Counting objects: 608151, done.\u001b[K\n","remote: Compressing objects: 100% (124916/124916), done.\u001b[K\n","remote: Total 608151 (delta 488547), reused 600207 (delta 481990)\u001b[K\n","Receiving objects: 100% (608151/608151), 142.37 MiB | 24.09 MiB/s, done.\n","Resolving deltas: 100% (488547/488547), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3MhgKTzdywC","executionInfo":{"status":"ok","timestamp":1602517282684,"user_tz":-330,"elapsed":2663,"user":{"displayName":"Amrit Pal Singh","photoUrl":"","userId":"02634295327518435662"}},"outputId":"dbd0a669-ea26-4878-8ad9-d0040c99c8e7","colab":{"base_uri":"https://localhost:8080/","height":315}},"source":["!ffmpeg"],"execution_count":5,"outputs":[{"output_type":"stream","text":["ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Hyper fast Audio and Video encoder\n","usage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}...\n","\n","\u001b[0;33mUse -h to get full help or, even better, run 'man ffmpeg'\n","\u001b[0m"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xSGJRjUJd5GO"},"source":[""],"execution_count":null,"outputs":[]}]}