{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izBv4OIqXSz2"
   },
   "source": [
    "## Throid segmentation - Grand challenges\n",
    "\n",
    "https://tn-scui2020.grand-challenge.org/Source_links/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tp1IpJrBXS0e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOkksff-XS1l"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks.hooks import *\n",
    "from fastai.utils.mem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tLlMoDjLY1Z"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision -y\n",
    "!pip install torch==1.4.0 torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQuRNjuDGex5"
   },
   "source": [
    "## Import dataset from Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nDbduR_iwDt"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/amritpal333/throid-segmentation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOrM9qJt1heQ"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp /content/kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTYASjkU1VAd"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d amritpal333/throid-segmentation\n",
    "\n",
    "!kaggle datasets download -d amritpal333/thyroid-segmentation-same-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wYluW_HS4oR"
   },
   "outputs": [],
   "source": [
    "!unzip -qq '/content/thyroid-segmentation-same-name.zip'\n",
    "\n",
    "#!unzip -qq '/content/throid-segmentation.zip'  -d '/' #takes 15 minutes\n",
    "#https://askubuntu.com/questions/458931/unpack-a-war-file-to-a-destination-folder-without-verbose-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xG3XQuVaqnjs"
   },
   "source": [
    "## Chest Xray masks\n",
    "https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 309171,
     "status": "ok",
     "timestamp": 1590768073125,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "Cldy8pGEqtzp",
    "outputId": "657b2025-5680-45a0-b3b8-21039b9e53ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading chest-xray-masks-and-labels.zip to /content\n",
      " 49% 4.73G/9.58G [03:51<07:09, 12.1MB/s]User cancelled operation\n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets download -d nikhilpandey360/chest-xray-masks-and-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwGme5b1XS3J"
   },
   "source": [
    "## Subset classes 子集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSm3gEn2XS3O"
   },
   "outputs": [],
   "source": [
    "# path = Path('./data/camvid-small')\n",
    "\n",
    "# def get_y_fn(x): return Path(str(x.parent)+'annot')/x.name\n",
    "\n",
    "# codes = array(['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', 'Tree',\n",
    "#     'Sign', 'Fence', 'Car', 'Pedestrian', 'Cyclist', 'Void'])\n",
    "\n",
    "# src = (SegmentationItemList.from_folder(path)\n",
    "#        .split_by_folder(valid='val')\n",
    "#        .label_from_func(get_y_fn, classes=codes))\n",
    "\n",
    "# bs=8\n",
    "# data = (src.transform(get_transforms(), tfm_y=True)\n",
    "#         .databunch(bs=bs)\n",
    "#         .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCppaXsd0yU7"
   },
   "source": [
    "## Data 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZE3B0mFXS2k"
   },
   "outputs": [],
   "source": [
    "path_img = '/content/00000TNSCUI2020_train/TNSCUI2020_train/image'\n",
    "path_lbl = '/content/00000TNSCUI2020_train/TNSCUI2020_train/mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3418,
     "status": "ok",
     "timestamp": 1590768899624,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "0mx-BFlu0yVi",
    "outputId": "fbabee63-c567-4f57-94ee-8687eadeded1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/image/9252.PNG'),\n",
       " PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/image/7996.PNG'),\n",
       " PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/image/380.PNG')]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = get_image_files(path_img)\n",
    "fnames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1590768899627,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "cYe0RVqy0yWO",
    "outputId": "17d6fb39-6da6-400c-9a73-0bc904ca36d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/mask/9252.PNG'),\n",
       " PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/mask/7996.PNG'),\n",
       " PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/mask/380.PNG')]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_names = get_image_files(path_lbl)\n",
    "lbl_names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3357,
     "status": "ok",
     "timestamp": 1590768899631,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "6k0IfstL9XH4",
    "outputId": "25f55220-0412-4017-b2f8-f503ac1e6dc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/image/7996.PNG')"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3329,
     "status": "ok",
     "timestamp": 1590768899636,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "IqwBUhnE9exg",
    "outputId": "18feb345-c182-47bb-fc01-914ea9487fa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/00000TNSCUI2020_train/TNSCUI2020_train/mask/7996.PNG')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5617,
     "status": "ok",
     "timestamp": 1590768901958,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "kLgdlo3f0yWp",
    "outputId": "d61a4c18-76ff-4ae3-ef43-d3fb4d5bd3fb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_f = fnames[0]\n",
    "img = open_image(img_f)\n",
    "img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zHRrtbX-h00"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxWlHQpbBeVB"
   },
   "outputs": [],
   "source": [
    "def get_y_fn(filename):\n",
    "   b = Path(path_lbl).joinpath(Path(filename).name)\n",
    "   return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4260,
     "status": "ok",
     "timestamp": 1590768901968,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "QCIHT4i0-T9k",
    "outputId": "60d3a62f-c494-416a-f111-33cfdebcc515"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAADqCAYAAADkrlOiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAE80lEQVR4nO3dwXHbRhiA0SijQtKC6kgZasI3jW9pwmWkDrfgTphTPLJMUqQEEt/uvnfz8AIQ2A8/YHD0cDgc/gCo+HPvDQB4TZSAFFECUkQJSBElIEWUgJTHcx/+9eVf7wsAm/vxz98Ppz4zKQEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpj3tvAPN6eX765d9fv33faUsYiSixqbchOvaZOHGOKC3gXCj28PL8JEycJEoTq8UILiFKkxEiRud/3ybx8vw0VJBG2lbuy6Q0sNEXtmdLHCNKgxk9RG8JE2+5fRvEaLdn15h1v/gYURrACot25uhyHVGKW22hihOeKUWtvjCP7b9nT2swKTEMU9QaRInhCNPcRCnIonufqWleohRjoV1HnOYjSkCKKIW44n+ciWkeohRhQW3D9zg+7yndmEVyf35PNzaT0g0J0n7czo1LlIAUt2834ird8Po4uKUbg0mJZbhQjEGUgBRRYimmpT5RYjnC1CZKN+Ck73OMukQJSBElIEWUNua2AD5HlFiWC0iTKG3Eb63G5Jj1+JnJGU5YuD+T0gmCBPsQpSMECfYjSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKLM1fOOkRpSOcqLAfUTpBmGAfogSkiBLLMg03idIZTlq4P1ECUkTpHaYluC9RAlJE6QKmpfk4pl2iBKSI0oVcWeE+RInluMC0iRKQIkpXcIWF2xMlIEWUrmRagtsSJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUWIo38vtE6QOc2HA7ogSkiNIHmZbG45iNQZSAFFH6BFde2N7j3hswuv/D9PL8tPOWcIqLx1hMShtx4sM2RGlDwtTjmIxHlDb29dt3CwE+wTOlG3kdJs+b4HImpTswPe3Ddz4mk9IdmZ7uR5DGZVLaiUVzO77bsYkSUxGk8YnSjjxrgt+JUoAwbcP3OAdRijA1fY7vbh6ixPAEaS5eCYg5tsC8PsBKRGkAl0wCq4bLlDQft2+TsDiZhShNZLUwrba/qxClyayyUFfZzxWJ0oRmf71g5n1DlKY2Y5xm2x9+J0oLmGEhzxhYjvNKwCJG/QMHQrQek9JiRlrkI20r2zEpLag8NQkRorSwwk9aRIi3RIlf3GuKEiNOESWOulWcxIj3iBJnbRUnMeJSosRFLo2T+PBZosRVRIdb854SkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKQ8HA6HvbcB4CeTEpAiSkCKKAEpogSkiBKQIkpAyn9e8fgblkan7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = open_mask(get_y_fn(fnames[0]))\n",
    "mask.show(figsize=(5,5), alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1590768901971,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "YpkphpTN0yXp",
    "outputId": "8c5a2a98-d1b9-470f-b495-f853853d4725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([600, 760]), tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_size = np.array(mask.shape[1:])\n",
    "src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3286,
     "status": "ok",
     "timestamp": 1590768903523,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "h4JAOPRj0yX8",
    "outputId": "0ed0ebce-5ae3-4bdf-ac73-d91e4b06107b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thyroid', 'other'], dtype='<U7')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes = np.loadtxt('/content/codes.txt', dtype=str); codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbsdDEu10yYQ"
   },
   "source": [
    "## Datasets 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1753,
     "status": "ok",
     "timestamp": 1590768909420,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "zGvLK57H0yYT",
    "outputId": "abf7fbee-38f5-49eb-dfd7-bba33e446f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bs=4, have 7601MB of GPU RAM free\n"
     ]
    }
   ],
   "source": [
    "size = src_size//2\n",
    "\n",
    "free = gpu_mem_get_free_no_cache()\n",
    "# the max size of bs depends on the available GPU RAM\n",
    "if free > 8200: bs=8\n",
    "else:           bs=4\n",
    "print(f\"using bs={bs}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfDpKF3TOBjj"
   },
   "outputs": [],
   "source": [
    "bs =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTUcMH_90yYs"
   },
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path_img)\n",
    "       .split_by_fname_file('/content/valid.txt')\n",
    "       .label_from_func(get_y_fn, classes=codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs-HhwHK0yY9"
   },
   "outputs": [],
   "source": [
    "data = (src.transform(get_transforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1748,
     "status": "error",
     "timestamp": 1590769418677,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "Q-DZbnbB0yZU",
    "outputId": "16776d13-4b8a-432c-bade-0f3418081174"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4672a3bbd9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m\"Show a batch of data in `ds_type` on a few `rows`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/THCCachingHostAllocator.cpp:278"
     ]
    }
   ],
   "source": [
    "data.show_batch(2, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1372,
     "status": "error",
     "timestamp": 1590768976810,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "H278mSLR0yZo",
    "outputId": "1d81c2b9-40c3-40d3-f354-b817e75feadd"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c3c4e9bb0955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m\"Show a batch of data in `ds_type` on a few `rows`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/THCCachingHostAllocator.cpp:278"
     ]
    }
   ],
   "source": [
    "data.show_batch(2, figsize=(10,7), ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Srbf3TrlXS8k"
   },
   "source": [
    "## Model 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IubhnI4XS87"
   },
   "outputs": [],
   "source": [
    "metrics=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCX-DqbdOZLx"
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pcj6lnjUXS9c"
   },
   "outputs": [],
   "source": [
    "wd=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4163,
     "status": "error",
     "timestamp": 1590769135112,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "JAYTeRVBXS9t",
    "outputId": "0c76a623-85f3-40eb-a33b-aef2cafcb4c2"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c273b30cf766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/vision/learner.py\u001b[0m in \u001b[0;36munet_learner\u001b[0;34m(data, arch, pretrained, blur_final, norm_type, split_on, blur, self_attention, y_range, last_cross, bottle, cut, **learn_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     model = to_device(models.unet.DynamicUnet(body, n_classes=data.c, img_size=size, blur=blur, blur_final=blur_final,\n\u001b[1;32m    122\u001b[0m           \u001b[0mself_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_cross\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m           bottle=bottle), data.device)\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlearn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(b, device)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"Recursively put `b` on `device`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mItemsList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/core.py\u001b[0m in \u001b[0;36mrecurse\u001b[0;34m(func, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfirst_el\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"Recursively put `b` on `device`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mItemsList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3917,
     "status": "error",
     "timestamp": 1590769071354,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "jqQ62D5-XS-B",
    "outputId": "ce96e058-63ec-4185-abe3-b9397dae9287"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-dd390b1c8108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_distrib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mcb_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, epochs, pbar, metrics)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcamel2snake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pbar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, pbar, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"Initialize optimizer and learner hyperparameters.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clean_on_interrupt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, file, return_path, with_opt)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwith_opt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/torch/csrc/generic/serialization.cpp:23"
     ]
    }
   ],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RHgIds6XS-R"
   },
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 613729,
     "status": "error",
     "timestamp": 1590768378465,
     "user": {
      "displayName": "Amrit Pal Singh",
      "photoUrl": "",
      "userId": "02634295327518435662"
     },
     "user_tz": -330
    },
    "id": "mzUMMVZfXS-h",
    "outputId": "b8bbf5d3-e9d5-4388-8135-878b47f142e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='639' class='' max='814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.50% [639/814 04:17<01:10 0.2854]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d53419f039b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m\"Handle gradient calculation on `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, slice(lr), pct_start=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeNjT2VoXS-0"
   },
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSkY5cc1XS_C"
   },
   "outputs": [],
   "source": [
    "learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iy364g8xXS_U"
   },
   "outputs": [],
   "source": [
    "learn.show_results(rows=3, figsize=(8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkV2IP3gXS_g"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nvmf2u6KXS_y"
   },
   "outputs": [],
   "source": [
    "lrs = slice(lr/400,lr/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ucgPGgCXTAE"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(12, lrs, pct_start=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NttYfddoXTAR"
   },
   "outputs": [],
   "source": [
    "learn.save('stage-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn0XLMkbXTAk"
   },
   "source": [
    "## Go big 用更大的数据集进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sS1loMmFXTAn"
   },
   "source": [
    "You may have to restart your kernel and come back to this stage if you run out of memory, and may also need to decrease `bs`.<br>\n",
    "如果内存不够的话，你可能需要重启你的计算内核，然后再返回这一步，同时可能要减少 `bs` 的设定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEXIV6XXXTAr"
   },
   "outputs": [],
   "source": [
    "learn.destroy()\n",
    "\n",
    "size = src_size\n",
    "\n",
    "free = gpu_mem_get_free_no_cache()\n",
    "# the max size of bs depends on the available GPU RAM\n",
    "if free > 8200: bs=3\n",
    "else:           bs=1\n",
    "print(f\"using bs={bs}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuKUHUuAXTBI"
   },
   "outputs": [],
   "source": [
    "data = (src.transform(get_transforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il2xDwWGXTBc"
   },
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIwlchhlXTB0"
   },
   "outputs": [],
   "source": [
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4Xizl2GXTCG"
   },
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7meKbe_gXTCV"
   },
   "outputs": [],
   "source": [
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XiIQG6CrXTCp"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, slice(lr), pct_start=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eNHjJ45XTDO"
   },
   "outputs": [],
   "source": [
    "learn.save('stage-1-big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PccO_cWgXTDr"
   },
   "outputs": [],
   "source": [
    "learn.load('stage-1-big');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6_-kn0aXTD-"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-lpozwsXTEO"
   },
   "outputs": [],
   "source": [
    "lrs = slice(1e-6,lr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDvB8gcGXTEf"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sO4qsyWMXTEq"
   },
   "outputs": [],
   "source": [
    "learn.save('stage-2-big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6l6XJu0XTEz"
   },
   "outputs": [],
   "source": [
    "learn.load('stage-2-big');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjN43KZ0XTFD"
   },
   "outputs": [],
   "source": [
    "learn.show_results(rows=3, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfSpPBFpTEl1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xG3XQuVaqnjs",
    "qwGme5b1XS3J",
    "wCppaXsd0yU7",
    "GbsdDEu10yYQ",
    "Srbf3TrlXS8k",
    "Fn0XLMkbXTAk"
   ],
   "name": "Throidsegmentation-Fastai .ipynb",
   "provenance": [
    {
     "file_id": "14r7Clt1qBeS5UV0-8UlQeU-g-DuQWa5V",
     "timestamp": 1590757463089
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
